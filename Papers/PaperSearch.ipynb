{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "274dc850-1374-49e7-88a4-be4ed7132af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"update\"\n"
     ]
    }
   ],
   "source": [
    "!pip update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ba1c86c-0ee8-4450-bcd4-736cd39d5bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install PyMuPDF\n",
    "#!pip install openai\n",
    "\n",
    "## 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd45e0cb-e199-43f6-8190-351af9e688da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n",
      "MuPDF error: syntax error: invalid key in dict\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'AIMC 2024 (09/09 - 11/09 )\\nAugmenting the\\nExpressivity of the\\nNotochord Generative\\nMIDI Model for Arca\\'s \"The\\nLight Comes in the Name\\nof the Voice\" Magnetic\\nResonator Piano\\nInstallation\\nPublished on: Aug 29, 2024\\nURL: https://aimc2024.pubpub.org/pub/0lh6s86c\\nLicense: Creative Commons Attribution 4.0 International License (CC-BY 4.0)\\nAIMC 2024 (09/09 - 11/09 )\\nAugmenting the Expressivity of the Notochord Generative MIDI\\nModel for Arca\\'s \"The Light Comes in the Name of the Voice\"\\nMagnetic Resonator Piano Installation\\n2\\nAbstract\\nGenerative MIDI models are increasingly prevalent as creative compositional aids and musical performance \\ntools, learning from large MIDI datasets to offer completions, continuations, in-painting and more. However, \\ntheir expressive capabilities are limited by the low fidelity and dimensionality of the MIDI protocol itself, and \\ndon\\'t perform as well in unstructured improvisatory scenarios due to being trained mainly on structured and bar-\\naligned compositions. We present an approach to overcoming these limitations, which emerged out of a \\ncollaboration with the artist Arca where her Magnetic Resonator Piano (MRP) performances were turned into a \\nseven-hour installation. The real-time probabilistic model Notochord learned in-context from Arca\\'s playing, \\nand via further structural constraints improvised new MIDI continuations. Arca\\'s continuous gestural data from \\nthe MRP were then re-mapped onto the new MIDI notes, creating endless renditions that retained gestural \\nsubtlety. We reflect on the various uncanny aspects of the installation and how this approach could be taken \\nfurther, and share code1 and notebooks2 documenting our process for others to build on.\\nAuthor Keywords\\nInstallation art, Magnetic Resonator Piano, Notochord, Artificial intelligence, Deep learning, Generative AI, \\nMIDI, Intelligent musical instruments\\nIntroduction\\nThis paper presents a novel exploration at the intersection of generative AI and musical performance, \\nspecifically through a collaboration that integrates the unique qualities of the Magnetic Resonator Piano (MRP) \\n[1] with the capabilities of generative MIDI models, in this case Notochord [2]. Highlighted in this project is an \\nArca presents The Light Comes in \\nthe Name of the Voice. Courtesy of \\nPinault Collection.\\nAIMC 2024 (09/09 - 11/09 )\\nAugmenting the Expressivity of the Notochord Generative MIDI\\nModel for Arca\\'s \"The Light Comes in the Name of the Voice\"\\nMagnetic Resonator Piano Installation\\n3\\ninstallation — MRP Day Activations — developed in collaboration with the artist Arca as part of her \\nmultifaceted event The Light Comes in the Name of the Voice, demonstrating the potential of integrating AI \\ninto instrumental performance to produce new forms of musical expression. The installation sought to \\nelaborate on recordings of Arca’s MRP performances, through a seven-hour continuous experience. Arca’s \\nevent served as a platform for artistic exploration and also as a contribution to the ongoing dialogue about the \\nrole of AI in pop music and live performance settings. In sharing our experience of designing the MRP \\ninstallation aspect of the event, we aim to shed light on the potential synergies between AI-generated content \\nand live musical expression.\\nOur contributions are the following:\\nWe begin the paper by describing the artistic context of the collaboration and our previous works with the \\nMRP. We then describe the MRP in more detail, and review generative AI in the context of MIDI generation, in \\nparticular contrasting existing models with Notochord, the model used in this project. Following this, we \\ndescribe the design and outcomes of the installation, and share our reflections and possible future directions.\\nArtistic Context\\nPrior MRP Work & Collaboration\\nThis project marks our first collaboration with Arca. Our research group is dedicated to examining the role of \\nartificial intelligence in the development of new musical instruments. The opportunity to work together arose \\nWe contribute to the practice of creative AI for pop music, reporting on a unique installation with Arca \\ncombining generative AI and the magnetic  resonator piano.\\nWe enhance accessibility to advanced musical interfaces by releasing the tools for playing the MRP using \\nthe Notochord MIDI model, alongside open-source notebooks and code for the wider MRP community.\\nWe report on our method of mapping expression curves to and from MIDI velocities to combine general \\nMIDI models with unique interfaces like the MRP.\\nWe provide reflections on the implications of our installation for the broader conversation on generative AI \\nwithin the pop music landscape.\\nThree views of the MRP Day Activations installation at Rotunda, Bourse de Commerce, Paris. Video credit: Lewis \\nWolstanholme.\\nAIMC 2024 (09/09 - 11/09 )\\nAugmenting the Expressivity of the Notochord Generative MIDI\\nModel for Arca\\'s \"The Light Comes in the Name of the Voice\"\\nMagnetic Resonator Piano Installation\\n4\\nin part due to us both having previously developed pieces for the MRP. In Arca’s case, this came in the form of \\nher concert held in New York in October 2023. As the New York Times reported, “[Arca’s] piano — unlike \\nSwift’s or Lady Gaga’s — is prepared with magnets that turn it into an electroacoustic machine of woozy, \\notherworldly lyricism, tinged with buzz.”3\\nMeanwhile, Armitage premiered the MRP-based installation Strengjavera at AIMC 2023. A second iteration \\nwas exhibited at Nordic House in Reykjavík, where the MRP was controlled by biomimetic artificial life \\nsimulations: “Bringing together the perfect intermingling of art and science, the new installation by composer, \\nproducer, performer and research Jack Armitage will have viewers asking big questions about humanity and \\ntechnology while being marvelled by beautiful acoustic piano sounds created in fascinating ways.”4 Further \\nrecent experimentation has involved training RAVE neural audio models on recordings of the installation [3].\\nArca performing with the MRP at Park Avenue Armory, \\nNew York, December 2023. Photography by Annie \\nForrest https://vmagazine.com/article/arcas-\\nmutantdestrudo-was-a-visual-manifesto-of-music-meets-\\ntech/\\n0:00\\nStrengjavera (2023): Installation for Magnetic Resonator \\nPiano and Self-Organising System.\\nAIMC 2024 (09/09 - 11/09 )\\nAugmenting the Expressivity of the Notochord Generative MIDI\\nModel for Arca\\'s \"The Light Comes in the Name of the Voice\"\\nMagnetic Resonator Piano Installation\\n5\\nFor The Light Comes in the Name of the Voice, Victor Shepardson joined the collaboration, providing expertise \\nin deep learning for real-time symbolic music generation, via the project Notochord. Iterative prototyping \\nhappened across sites in Paris and Reykjavík. The group aimed to capture the essence of Arca’s MRP playing \\nand extend it for a seven hour installation in Bourse de Commerce’s Rotunda, supervised by onsite technician \\nLewis Wolstanholme. The group paid particular attention to elevating and honouring Arca’s artistic intentions \\nover the MRP’s technical constraints, while also remaining open to the generative expressive potential of \\nmachine perturbation.\\nThe Light Comes in the Name of the Voice\\nBelow we provide an excerpt of the public event description5:\\nArca presents her new musical exploration, “The Light Comes in the name of the Voice” at the Bourse de \\nCommerce6. In a diffuse and ghostly manner, Arca previously inhabited the museum’s Rotunda in 2022 \\nfor the Echo2 installation by Philippe Parreno, presented on the occasion of the exhibition “Une seconde \\nd’éternité.” The unique acoustics of this monumental space, formed by the union of the 19th-century \\nHalle aux blés and Tadao Ando\\'s concrete cylinder, offer her a distinctive echo chamber and a new \\nplayground.\\nProduced specifically for this space, “The Light Comes in the name of the Voice” begins as a minimal \\nquest for sound transformation. It is first formed between Arca and a magnetic resonated piano -an \\nelectronically-augmented acoustic instrument that produces new acoustic sounds from the piano strings, \\ninducing vibrations to create countless crescendos and harmonics, all controlled from the keyboard. \\nDuring the daytime, as Arca disappears, the magnetic resonator piano (MRP) continues to play using AI \\ntechnology trained on Arca\\'s playing.\\nTwo photos of the Rotunda, Bourse de Commerce, Paris, France. Left: photograph of the \\nRotunda by Jean-Pierre Dalbéra taken on June 25, 2021, CC-BY-2.0 on Flickr. Right: \\nphotograph of the stage and MRP being set up, photograph by Lewis Wolstanholme.\\nAIMC 2024 (09/09 - 11/09 )\\nAugmenting the Expressivity of the Notochord Generative MIDI\\nModel for Arca\\'s \"The Light Comes in the Name of the Voice\"\\nMagnetic Resonator Piano Installation\\n6\\nBackground\\nMagnetic Resonator Piano\\nMRP inventor Andrew McPherson introduces his creation this way7:\\nThe magnetic resonator piano (MRP) is an augmented piano which uses electromagnets to elicit new \\nsounds from the strings of a grand piano. The MRP extends the sonic vocabulary of the piano to include \\ninfinite sustain, crescendos from silence, harmonics, pitch bends and new timbres, all produced \\nacoustically without the use of speakers. The MRP is an electronic instrument, but the experience of \\nplaying it closely resembles the acoustic piano it inherits from. An optical scanner on the piano keyboard \\nmeasures the continuous position of every key, enabling subtle, delicate gestures that would not produce \\nsound on an ordinary piano, even as all the techniques of traditional piano playing remain available. The \\nkit can be installed in any grand piano. The Augmented Instruments Laboratory website8 features more \\ndetails about the research and technology behind the MRP. \\nDescribing the origins of the MRP, McPherson recalled “what I had in mind was the idea that you would have \\nsomething that was still recognizable as a piano but had a sort of extraordinary range of tone color, especially \\nthe idea that you can separate the timbre from the dynamics.” [4]\\nArca performing at the MRP during The Light Comes in the Name of the Voice. Courtesy of \\nPinault Collection. Photographer: Léonard Méchineau (https://www.leonardmechineau.com/)\\nAIMC 2024 (09/09 - 11/09 )\\nAugmenting the Expressivity of the Notochord Generative MIDI\\nModel for Arca\\'s \"The Light Comes in the Name of the Voice\"\\nMagnetic Resonator Piano Installation\\n7\\nVisit the web version of this article to view interactive content.\\nMcPherson and Kim [5] detail the gesture-sound mapping of the MRP in a 2012 paper, reproduced below, \\nwhich is crucial to understanding how to work with data produced by the MRP.\\nMagnetic Resonator Piano. Left: top-down. Right: electromagnets detail. Photos courtesy of \\nAndrew McPherson.\\nMagnetic Resonator Piano demonstration video.\\nAIMC 2024 (09/09 - 11/09 )\\nAugmenting the Expressivity of the Notochord Generative MIDI\\nModel for Arca\\'s \"The Light Comes in the Name of the Voice\"\\nMagnetic Resonator Piano Installation\\n8\\nUnlike the majority of new musical instruments, the MRP features a custom recording format, and the ability to \\nrecord and playback performances. The below listing shows an example of MRP OSC recording data. Here, \\ntwo notes (MIDI 62 & 56) are played, and their intensity and pitch vibrato parameters are modulated, and the \\ntimestamps (in seconds) show the fine resolution of data:\\nFigure and quote from The Problem of the Second Performer: Building a Community Around \\nan Augmented Piano, McPherson & Kim 2012: ”The standardized mapping uses a two-level \\nstrategy:\\n1. Features of key motion are mapped to four intermediate parameters: intensity, brightness, \\nharmonic, and pitch. The parameters, whose meanings are explained in the following, are \\nquasi-independent in that not all combinations are possible but none is a linear combination of \\nthe others.\\n2. Intermediate parameters are mapped to actuator behavior, controlling amplitude, frequency \\n(relative to string fundamental), and waveform (combinations of harmonics, affecting \\namplitudes, phases, and centroid in Figure 5).\\nTwo-layer mapping from key motion to actuator parameters. The second layer is dynamically \\nadjustable; the standardized mapping is shown in solid black lines.”\\nAIMC 2024 (09/09 - 11/09 )\\nAugmenting the Expressivity of the Notochord Generative MIDI\\nModel for Arca\\'s \"The Light Comes in the Name of the Voice\"\\nMagnetic Resonator Piano Installation\\n9\\n0.144359 /mrp/midi iii 159 62 127 \\n0.148741 /mrp/quality/intensity iif 15 62 0  \\n0.150149 /mrp/quality/intensity iif 15 62 0  \\n0.150253 /mrp/midi iii 159 56 127 \\n0.151611 /mrp/quality/intensity iif 15 56 0  \\n0.151668 /mrp/quality/intensity iif 15 62 0  \\n0.154576 /mrp/quality/intensity iif 15 56 0  \\n0.154654 /mrp/quality/intensity iif 15 62 0  \\n0.157414 /mrp/quality/intensity iif 15 56 0  \\n0.157479 /mrp/quality/intensity iif 15 62 0  \\n0.160273 /mrp/quality/intensity iif 15 56 0  \\n0.160335 /mrp/quality/intensity iif 15 62 0  \\n0.163375 /mrp/quality/intensity iif 15 56 0  \\n0.163547 /mrp/quality/intensity iif 15 62 0  \\n0.166477 /mrp/quality/intensity iif 15 56 0  \\n0.166749 /mrp/quality/intensity iif 15 62 1  \\n0.168951 /mrp/quality/intensity iif 15 56 1  \\n0.168989 /mrp/quality/intensity iif 15 62 1  \\n0.171918 /mrp/quality/intensity iif 15 56 1  \\n0.17199 /mrp/quality/intensity iif 15 62 1  \\n0.174787 /mrp/quality/intensity iif 15 56 1  \\n0.174844 /mrp/quality/intensity iif 15 62 1  \\n0.177677 /mrp/quality/intensity iif 15 56 1  \\n0.177761 /mrp/quality/intensity iif 15 62 1  \\n0.179157 /mrp/quality/pitch/vibrato iif 15 62 0  \\n0.179223 /mrp/quality/intensity iif 15 56 1  \\n0.179245 /mrp/quality/intensity iif 15 62 1  \\n0.180599 /mrp/quality/pitch/vibrato iif 15 62 0  \\n0.180645 /mrp/quality/intensity iif 15 56 1  \\n0.180665 /mrp/quality/intensity iif 15 62 1  \\n0.182038 /mrp/quality/pitch/vibrato iif 15 62 0  \\n0.182082 /mrp/quality/intensity iif 15 62 1  \\n0.183462 /mrp/quality/pitch/vibrato iif 15 62 0 \\nDue to the aforementioned gesture-sound mapping, by working with the recording data rather than the \\nKeyScanner data itself, we are working at a few abstraction levels away from direct gestural data, mediated by \\nthe MRP software’s per-note state machines. The MRP’s subtle, micro scale details [6] emerge from a complex \\ninteraction between the physical piano, the KeyScanner and its calibration, the software’s state machines, and \\nthe performer themselves.\\nMost MRP repertoire falls in the contemporary classical tradition, with pianists extending their existing \\ntechniques, and emulation software has been written to support this practice [7]. To facilitate artistic departure \\nfrom existing MRP repertoire, we have developed iimrp9. The repository contains client software that \\nimplements the MRP’s protocols in Python, SuperCollider, Max/MSP and TidalCycles. The software has been \\ndeveloped to enable new ways of interacting with the MRP that go beyond traditional keyboard-based control. \\nMore examples of iimrp usage can be found in our examples repository10.\\nGenerative MIDI Models\\nProbabilistic generative music has a long history with roots in the Markov models of the Iliac Suite and the \\nStochastic music of Xenakis, and reaching all the way back to ancient methods of divination. In recent years, \\nsubstantial research has sought to deploy data driven ‘deep learning’ models to generative modeling of \\nsymbolic music and musical signal. Much effort has focused on MIDI, since large amounts of MIDI music can \\nbe obtained as training data, yet the modeling task is generally simpler than for raw audio. Some efforts model \\nfull tracks with multiple parts, while others focus on solo performance or even monophonic generation. \\nAIMC 2024 (09/09 - 11/09 )\\nAugmenting the Expressivity of the Notochord Generative MIDI\\nModel for Arca\\'s \"The Light Comes in the Name of the Voice\"\\nMagnetic Resonator Piano Installation\\n10\\nMeanwhile, some efforts have treated MIDI more like a score, imposing keys, time signatures and strict \\nquantization of events, while other works attempt to model MIDI files as performances including variable \\ntiming and gesture.  Ji et al. [8] call the latter “composing expressive performance” while Oore et al. [9] use the \\nterm “direct performance generation”. Amongst these, though some systems are designed explicitly for \\ninteractivity [10], most are not designed for low-latency generation. Live performance, if possible at all, tends \\nto involve an asynchronous process of loop generation or variation. \\nNotochord\\nThe Notochord [2] model was designed to be the first deep generative MIDI performance model to meet three \\ndesign goals at once:\\nGoal 1 means that systems using Notochord can integrate into a real-time performance with little perceptible \\ndelay — for example, the model can react instantly to change of key or dynamics by a performer, without \\nwaiting for a bar boundary or indeed needing a notion of time signature. To achieve this, the model must run \\nquickly on a CPU, and must represent notes using NoteOn and NoteOff events — note durations can’t be \\nconditioned on with low latency, since they aren’t known until the note is over.\\nGoal 2 means that a single Notochord model can generate full polyphonic performances with multiple \\ninstruments. To achieve this, the model must represent the part, channel or instrument associated with each \\nevent.\\nGoal 3 means that a performer can take very fine-grained control over generation — for example, Notochord \\ncan predict pitches conditioned on the timing and velocity from a MIDI controller; or Notochord can play two \\ninstruments autonomously as the performer plays a third; or Notochord can generate music while a performer \\nconstrains the allowable pitches, degree of polyphony, dynamics and instrumentation.\\n1. Very low latency (~10ms) processing of MIDI events appropriate for real-time performance\\n2. Multi-part modeling of full tracks\\n3. Constrained prediction of each note event attribute (pitch, velocity, instrument, timing) conditional on any \\nother attributes\\nAIMC 2024 (09/09 - 11/09 )\\nAugmenting the Expressivity of the Notochord Generative MIDI\\nModel for Arca\\'s \"The Light Comes in the Name of the Voice\"\\nMagnetic Resonator Piano Installation\\n11\\n α1 = \\n\\x00 \\n p1 = E3\\nΔt 1 = 50ms\\n v 1 = 99\\n α0 = <s t art > \\n p0 = <s t art >\\nΔt 0 =  0\\n v 0 =  0\\nh 1\\nh 0\\nGRU\\nGRU\\nf h\\nf α\\nP(α1)  P(p1)  P(Δt 1)   P(v 1)\\nf p\\nf t\\nf v\\nIn this project, we used a Notochord model trained on the lakh MIDI dataset11, a large collection of MIDI files \\nfound on the internet including representations of pop, classical and soundtrack music. The same model was \\nused in the Notochord Arcs and Scrambled Signals performance at AIMC 2023 [11]. It often creates a feeling \\nof being at the edge of sense, making errors and non-sequiturs while also developing and re-contextualising \\nthem, wandering between styles. \\nNotochord is distributed as a Python package via PyPI12 and is available on GitHub13. Notochord models can \\nbe used via a Python API or OSC server, and there are also several real-time MIDI processing apps.\\nFigure and caption quoted from \\nthe original Notochord paper: \\n“Architecture of the Notochord \\nmodel at training time. \\nRectangular blocks are functions, \\nlong capsules are embedding \\nvectors, and short capsules are \\nhidden states. Each sub-event \\ndepends on previous events via \\na GRU, and also on a random \\nsubset of the other sub-events. \\nConditioning of each sub-event \\non other sub-events is achieved \\nby simply adding their \\nembeddings to the hidden state \\nafter passing it through an MLP. \\nThe addition can be implemented \\nin parallel as a batched matrix \\nmultiplication at training time. \\nThis is depicted with black cells \\nindicating a one, gray cells a \\nrandom binary, and white cells a \\nzero. A final MLP per sub-event \\nmaps the summed embeddings \\nand hidden states to distribution \\nparameters. MLP architecture is \\nshown as an inset, top right.” \\nAIMC 2024 (09/09 - 11/09 )\\nAugmenting the Expressivity of the Notochord Generative MIDI\\nModel for Arca\\'s \"The Light Comes in the Name of the Voice\"\\nMagnetic Resonator Piano Installation\\n12\\n“MRP Day Activations” Installation Design\\nArtist Brief & Constraints\\nThe aim of this project was to create an AI system that could improvise in the style of Arca\\'s performances for \\nplayback on the MRP, for a duration of around seven hours, during the daytime between the two evening \\nconcerts. Given the extremely tight timeline—less than three days from receiving performance data to the \\ninstallation—and the remote nature of our collaboration, we faced several challenges. Below we describe these \\nconstraints and how they led to the conclusion that Notochord was an excellent fit for this project. \\nThe first data we received was of Arca’s rehearsal with the MRP. Playing it back on our MRP felt uncanny, as \\nthe piano hammers, which would have been activated by Arca originally, were for us motionless. Upon \\nanalyzing the data, we recognized the difficulty in applying traditional melody- and meter-focused \\ntechnologies due to the performance\\'s complex polyphony and free timing. In addition, Arca specifically \\ncalibrates the MRP’s KeyScanner sensor bar in a unique way. Usually, calibration is performed by pressing \\neach individual key down to the key bed, but Arca calibrates the MRP with only light touches, such that \\nrelatively small key depressions result in full intensity notes, enabling light and fluidic stroking gestures. \\nVisualisation of MRP recording of Arca’s rehearsal data, shown as notes \\nand expression curves over time. The intensity (red) parameters’ fast \\nattack and saturation is seen, which is in fact a consequence of the MRP’s \\nnote-level state machine, where intensity is set to 1 when player gestures \\ntrigger vibrato, brightness and harmonic qualities, as was frequently the \\ncase with Arca. Musically, the visualisation demonstrates the highly \\npolyphonic and rubato nature of the artist’s playing style. Also noteworthy \\nis the diverse variation across compositional episodes, with particular \\ncontrast between long, held chords, and rapid arpeggiations facilitated by \\nthe choice of extra sensitive MRP calibration. (150707)\\nAIMC 2024 (09/09 - 11/09 )\\nAugmenting the Expressivity of the Notochord Generative MIDI\\nModel for Arca\\'s \"The Light Comes in the Name of the Voice\"\\nMagnetic Resonator Piano Installation\\n13\\nIt would also be difficult to train new models given the short time available for iteration and the small amount \\nof new training data. Instead, we looked for a pre-trained model which could learn from the new data in-\\ncontext [12] and which would allow enough control for us to shape an aesthetically interesting result. Since we \\nalso worked remotely, we did not attempt to design a real-time system, but rather pre-rendered performances to \\nbe played back by the MRP. However, it was advantageous to iterate using a model capable of generating faster \\nthan real-time on a laptop, and with which we were already familiar.  These considerations led us to the choice \\nof Notochord as a generative model for the project. We considered other recent off-the-shelf MIDI models [13]\\n[14], but found that most assume structured compositions, which didn\\'t suit the free-form nature of Arca\\'s \\nperformance.\\nNotochord is a model for MIDI note events, but an MRP performance — particularly in Arca’s style of \\nplaying, with the keys tuned to very high sensitivity —  is as much in the expression of each note via intensity, \\nbrightness and vibrato gestures as in the notes themselves.  modeling expression data directly seemed \\nimpractically complex given our constraints. So, the challenge remained of mapping MRP expression onto a \\nform that Notochord could interpret.\\nImplementation\\nWe used two strategies for shaping the Notochord performance: in-context learning, and structural constraints. \\nFirst, we fed the whole original performance into a Notochord model as a prompt, so that the generated \\nmaterial would function as a continuation of the original performance. This served to inform Notochord \\nimplicitly about the general characteristics of the performance —  harmony, tempo, timing, and so on. Second, \\nwe generated continuations by constraining each new MIDI event to correspond roughly to an event from the \\noriginal sequence. Afterwards, we mapped expression curves back onto the new continuation.\\nRecord\\nRecord\\xa0MRP\\nMRP \\xa0\\n Performance\\nPerformance\\nRecord\\xa0MRP \\xa0\\n Performance\\nExport\\nExport\\xa0MRP\\nMRP \\xa0\\n Recording\\nRecording\\nExport\\xa0MRP \\xa0\\n Recording\\nPlayback\\nPlayback \\xa0\\n on\\non\\xa0MRP\\nMRP\\nPlayback \\xa0\\n on\\xa0MRP\\nParse\\nParse \\xa0MRP\\nMRP \\xa0\\n Recording\\nRecording\\nParse \\xa0MRP \\xa0\\n Recording\\nCompute\\nCompute \\xa0Velocity\\nVelocity \\xa0\\n Scores\\nScores\\xa0Per\\nPer\\xa0Note\\nNote\\nCompute \\xa0Velocity \\xa0\\n Scores\\xa0Per\\xa0Note\\nConvert\\nConvert\\xa0to\\nto\\xa0\\n Notochord\\nNotochord\\xa0Events\\nEvents\\nConvert\\xa0to\\xa0\\n Notochord\\xa0Events\\nFeed\\nFeed\\xa0Events\\nEvents\\xa0\\n as\\nas\\xa0Prompt\\nPrompt\\nFeed\\xa0Events\\xa0\\n as\\xa0Prompt\\nGenerate\\nGenerate\\xa0\\n Continuation\\nContinuation\\nGenerate\\xa0\\n Continuation\\nResample\\nResample \\xa0\\n Expression\\nExpression \\xa0Curves\\nCurves\\nResample \\xa0\\n Expression \\xa0Curves\\nNotochord\\nNotochord\\nNotochord\\nContinuation Generation\\nEach event generated by Notochord was within, for example, one octave of the original, 32 velocity units, and \\nhalf to double the elapsed time since the previous event. The sequence of NoteOn and NoteOff events was the \\nsame, though the model was free to match NoteOffs to NoteOns originally of different pitches. Specifically for \\nthe MRP, we also built-in constraints to prevent magnets from overheating due to being held for too long. \\nProbabilistic models like Notochord can incorporate such constraints seamlessly — at any given time, if there \\nBlock diagram of the process from recording of human MRP \\nperformance, through to playback of generated continuation on MRP.\\nAIMC 2024 (09/09 - 11/09 )\\nAugmenting the Expressivity of the Notochord Generative MIDI\\nModel for Arca\\'s \"The Light Comes in the Name of the Voice\"\\nMagnetic Resonator Piano Installation\\n14\\nare pitches which have been activated for too much cumulative time, the model is constrained to choose only \\nthose pitches for NoteOff events, and prevented from choosing them for NoteOn, simply by setting certain \\nprobabilities to zero and normalizing the resulting distribution. \\nThe result of this method was a new improvisation following the rough texture of the original and \\ncomplementing it in general harmony and timing characteristics. Below we provide a pseudocode version of \\nthis processing stage, corresponding to ‘Generate Continuation’ in the block diagram above, to complement the \\ncomplete Python notebook:\\ndef noto_variation(model, original_events, heat_thresh) \\n    model.reset() \\n \\n    all_pitches = {e[\\'pitch\\'] for e in original_events} \\n    events = [] # store generated events \\n    held_pitches = set() # track which notes are playing \\n    pitch_heat = {p:0 for p in all_pitches} # track cumulative heat of magnets \\n     \\n    # For each event in the original performance: \\n    for event in original_events: \\n        hot_pitches = { \\n          p for p,h in pitch_heat.items()  \\n          if h > heat_thresh} \\n       \\n        # if it is a NoteOn \\n        if event[\\'vel\\'] > 0:         \\n            allowed_pitches = ( \\n              all_pitches  \\n              & set(range(event[\\'pitch\\']-12, event[\\'pitch\\']+13))  \\n              - held_pitches  \\n              - hot_pitches \\n            ) \\n            new_event = model.query_feed( \\n              min_time=event[\\'time\\']/2,  \\n              max_time=event[\\'time\\']*2, \\n              min_vel=max(1, event[\\'vel\\']-16), \\n              max_vel=min(127, event[\\'vel\\']+16), \\n              include_pitch=allowed_pitches \\n            ) \\n        else: \\n            allowed_pitches = hot_pitches or held_pitches \\n            new_event = model.query_feed( \\n              min_time=event[\\'time\\']/2,  \\n              max_time=event[\\'time\\']*2, \\n              next_vel=0, \\n              include_pitch=allowed_pitches \\n            ) \\n \\n        # accumulate heat \\n        events.append(new_event) \\n        for p in held_pitches: \\n            pitch_heat[p] += new_event[\\'time\\'] \\n        for p in all_pitches - held_pitches: \\n            pitch_heat[p] -= new_event[\\'time\\'] \\n \\n        # update held notes \\n        if new_event[\\'vel\\'] > 0:     \\n            held_pitches.add(new_event[\\'pitch\\']) \\n        else: \\n            held_pitches.remove(new_event[\\'pitch\\']) \\n \\n        events.append(new_event) \\n         \\n    return events\\nAIMC 2024 (09/09 - 11/09 )\\nAugmenting the Expressivity of the Notochord Generative MIDI\\nModel for Arca\\'s \"The Light Comes in the Name of the Voice\"\\nMagnetic Resonator Piano Installation\\n15\\nExpression Curve Mapping\\nNotochord models note velocities, but the MRP doesn’t use the velocities associated with each MIDI event — \\ninstead dynamics are present in a continuous intensity curve. We reduced expression curves to a scalar \\n‘velocity score’ for each MRP note based on the average value of the intensity and other expression curves, and \\nsent that to the Notochord model. When generating new notes from the model, MIDI velocity was mapped \\nback to an original expression curve from the data by selecting the curve with the nearest velocity score. \\nFinally, the selected expression curve was adapted to the length of the generated note by time-stretching while \\npreserving the original attack speed, i.e. not stretching the first 200 milliseconds. The ensured that intensity \\ncurves would attack in a manner similar to the original performance, but vibrato curves wouldn’t be truncated \\nfrom the end of a note, where they often bend toward an adjacent pitch in a legato gesture.\\nOutcomes\\nThe simple approach of mapping between note expressions and velocities was more expedient than building a \\nparametric model for the MRP expression curves, while still allowing Notochord to make sense of dynamics. \\nReusing actual curves from the recording propagated some of the expressive feeling of Arca’s performance, if \\nscrambled in the process of being made legible to the model. We queried Notochord for several variations of \\nthe original performance, using a variety of specific versions of the structural constraints and temperature \\nparameters. The final installation stitched together several such generations into a piece of around one hour \\nduration, which was looped for the seven hour installation.\\nExpression data for one single note from an MRP performance\\nAIMC 2024 (09/09 - 11/09 )\\nAugmenting the Expressivity of the Notochord Generative MIDI\\nModel for Arca\\'s \"The Light Comes in the Name of the Voice\"\\nMagnetic Resonator Piano Installation\\n16\\nA Synthesis of Self States\\nThere was one final challenge that emerged when we received the MRP recording data from the first concert, \\nand were tasked with generating more continuations based on this before the installation opened. The concert \\ndata contrasted starkly with the rehearsal data in the following ways:\\nAs it turned out, Arca’s MRP performance was integrated into a complex performance including multiple other \\ninstruments and effects pedals, and live recording and processing of the MRP across a spatial audio system. \\nThere were also bright stage lights potentially interfering with the MRP’s key height sensors, which may have \\nalso been interacting with Arca’s extra sensitive MRP calibration setting. Fortunately, we were able to make \\nuse of this data because Notochord’s generations are resilient against sequences of extremely short notes. We \\ndetected and reduced the large gaps and filtered out events of <50ms duration, because these would not allow \\nenough time for the magnet to resonate the string. For our final generations, we concatenated the rehearsal and \\nconcert data to provide more compositional variety, which Arca likened to a “synthesis of different self states”.\\nDiscussion\\nOur system produced intimate yet uncanny generations of new MRP data to be played back on the hardware. \\nElements of the harmony and expressive timing of the original performance remained, draped across its gross \\nstructure, yet the machine performance seemed to bristle in every direction, played by many hands and many \\nminds.\\nReflections on “MRP Day Activations”\\nIn MRP Day Activations, the AI-generated performance emerged as a distinct reinterpretation of the original, \\nimbued with a sense of uncanniness. It sounded like a version of the original performance, but possessed an \\nexpressive set of gestural curves that, at times, defied logic, with vibratos bending in unexpected directions. \\nDespite adhering to the overall form, it presented music beyond the capabilities of a single human pianist. \\nNevertheless, we felt it offered a peculiar temporal and expressive coherence of its own, while clearly not \\nplayed by a person with two hands.\\nThe transition from expressive curves to velocity scores resulted in a degree of abstraction and unpredictability \\nin the conversion back to expressive parameters. This process also underscored various dimensions of \\nuncanniness, and a disjunction of expression in the absence of temporal continuity. Given the choice to “clean” \\nthe performance data further by for example removing shorter notes, we opted not to, out of respect for Arca’s \\n1. The data was temporally sparse, with gaps sometimes minutes in length between episodes.\\n2. Many notes had a duration of 0-100ms, which were unlikely to have been played physically by rapidly \\ndepressing and releasing a key.\\n3. Compositionally there were no long, held chords.\\nAIMC 2024 (09/09 - 11/09 )\\nAugmenting the Expressivity of the Notochord Generative MIDI\\nModel for Arca\\'s \"The Light Comes in the Name of the Voice\"\\nMagnetic Resonator Piano Installation\\n17\\noriginal brief and the performance itself. In the end we felt that these elements resonated with Arca\\'s aesthetics, \\nchallenging traditional perceptions of musical performance and embodiment.\\nThe project\\'s reliance on remote collaboration and the specific format of the MRP\\'s OSC recording \\nunderscored the potential for innovative and ecologically sensitive methodologies within the New Interfaces \\nfor Musical Expression (NIME) community. The existence of the recording format facilitated a unique \\ndialogue over time and distance, between Arca\\'s distinctive playing style and the generative capabilities of the \\nNotochord model to refract decades of MIDI culture. Striking a balance between Notochord’s uncanny quality \\nand the original\\'s essence became a subtly artistic contribution, more than just a technical one. Left to its own \\ndevices, Notochord can at times spiral into drunken mind-wandering and myopic repetition, which we have \\nexplored artistically in other contexts described earlier. Being familiar with this, we became attuned to when \\nthe generated continuations became “too Notochord-y” and tweaked parameters accordingly. \\nFuture Work\\nThere are several avenues for future exploration and development of the techniques described in this paper. \\nThese considerations stem not only from the project\\'s outcomes but also from the evolving landscape of AI \\nparticularly in instrumental musical expression [15].\\nIn-Context Learning\\nFuture developments could explore the refinement of input MIDI data processing, testing different techniques \\nsuch as chunking, clustering, or hierarchical analysis to confer different representations of the musical data, \\nresulted in different generated outcomes. A more nuanced approach may enable more sophisticated in-context \\nlearning mechanisms, thereby distinguishing further from offline fine-tuning by adapting dynamically to the \\nevolving musical landscape within a performance.\\nReal-Time Augmented MIDI Expression\\nDespite Notochord being a real-time model, the time constraints and remote collaboration in this scenario \\nmeant that an offline-first approach was preferable. The techniques we have described however are all \\napplicable to low-latency real-time performance, with the exception of our method of time-stretching \\nexpression curves. In order to preserve the ends of expression curves, we could instead delay NoteOff events \\nslightly, which we anticipate would work well for the MRP with its gradual release times. Or, when the legato \\ngesture is triggered by playing adjacent notes, we could instead model vibrato curves of generated notes more \\ndeterministically following the MRP software. \\nRegardless of the preferred approach, a real-time version of our system could learn to ornament or continue a \\nperformance interactively while a performer is at the piano, allowing investigation of uncanniness from the \\nperspective of the performer. Further, if a larger dataset of MRP performances can be collected, we might \\nAIMC 2024 (09/09 - 11/09 )\\nAugmenting the Expressivity of the Notochord Generative MIDI\\nModel for Arca\\'s \"The Light Comes in the Name of the Voice\"\\nMagnetic Resonator Piano Installation\\n18\\nmatch expression curves to context based on more than a unidimensional velocity score, considering pitch or \\nother aspects such as recent note duration or degree of polyphony.\\nFrom Static to Dynamic MIDI Model Constraints\\nAs we contemplate the transition to real-time systems, the concept of dynamically adjusting generative \\nconstraints based on performance context becomes increasingly relevant. This adaptive approach could \\nrespond to various factors, such as performance dynamics, temporal shifts, or the evolving emotional landscape \\nof a piece, increasing the fidelity and responsiveness of the performer’s dialogue with the model, instrument \\nand audience.\\nConclusion\\nThis paper has detailed a novel approach to enhancing the expressive capabilities of generative MIDI models, \\nparticularly within unstructured improvisational contexts where existing models may struggle. Our \\ncollaboration with Arca, utilizing her magnetic resonator piano (MRP) performances, served as a fertile ground \\nfor this exploration. The project culminated in a seven-hour installation that showcased the potential of real-\\ntime probabilistic modeling to generate MIDI continuations that are inventive and expressive, while remaining \\nfaithful to a specific artist’s style.\\nBy leveraging Notochord, a model adept at learning in-context from Arca\\'s unique playing style, we introduced \\nstructural constraints that enabled the improvisation of new MIDI sequences. These were not mere algorithmic \\noutputs; they were imbued with the essence of the original performances through the remapping of Arca\\'s \\ngestural data onto the generated MIDI notes. This process resulted in performances that, while uncanny, \\nmaintained a profound connection to the gestural nuances characteristic of Arca\\'s MRP playing.\\nAcknowledgements\\nThanks to Arca, Shaun MacDonald, Lewis Wolstanholme, Andrew McPherson, Bronze.ai, Google Arts & \\nCulture and the onsite teams at Bourse de Commerce, Paris.\\nThe Intelligent Instruments project (INTENT) is funded by the European Research Council (ERC) under the \\nEuropean Union’s Horizon 2020 research and innovation programme (Grant agreement No. 101001848).\\nEthics Statement\\nThis project is funded by the European Research Council (ERC) and before embarking upon our research we \\nsought ethical clearance from Iceland’s Science Ethics Committee. The committee considered our research \\nquestions, methods, recruitment strategies and treatment of personal data. All personal information is kept \\nsafely and anonymously.\\nAIMC 2024 (09/09 - 11/09 )\\nAugmenting the Expressivity of the Notochord Generative MIDI\\nModel for Arca\\'s \"The Light Comes in the Name of the Voice\"\\nMagnetic Resonator Piano Installation\\n19\\nReferences\\nFootnotes\\nReferences\\n1.  https://github.com/Intelligent-Instruments-Lab/iimrp ↩\\n2.  https://github.com/Intelligent-Instruments-Lab/iil-\\nexamples/blob/main/iimrp/mrp_notochord_continuations.ipynb ↩\\n3.  https://www.nytimes.com/2023/10/12/arts/music/review-arca-mutant-destrudo-armory.html ↩\\n4.  https://nordichouse.is/en/event/strengjavera-by-jack-armitage/ ↩\\n5.  https://www.pinaultcollection.com/en/boursedecommerce/arca-presents-light-comes-name-voice ↩\\n6.  https://en.wikipedia.org/wiki/Bourse_de_commerce_(Paris) ↩\\n7.  https://andrewmcpherson.org/project/mrp ↩\\n8.  https://instrumentslab.org/research/mrp.html ↩\\n9.  https://github.com/Intelligent-Instruments-Lab/iimrp ↩\\n10.  https://github.com/Intelligent-Instruments-Lab/iil-examples/tree/main/iimrp ↩\\n11.  https://colinraffel.com/projects/lmd/ ↩\\n12.  https://pypi.org/project/notochord/ ↩\\n13.  https://github.com/Intelligent-Instruments-Lab/notochord ↩\\n1. McPherson, A. (2010). The Magnetic Resonator Piano: Electronic Augmentation of an Acoustic Grand \\nPiano. Journal of New Music Research, 39(3), 189–202. https://doi.org/10.1080/09298211003695587 ↩\\n2. Shepardson, V., Armitage, J., & Magnusson, T. (2022). Notochord: a Flexible Probabilistic Model for \\nEmbodied MIDI Performance. https://doi.org/10.5281/zenodo.7088404 ↩\\n3. Shepardson, V., Armitage, J., Privato, N., & Crozzoli, M. (2023).  rave-models . Retrieved from \\nhttps://huggingface.co/Intelligent-Instruments-Lab/rave-models ↩\\n4. Armitage, J. (2022). Subtlety and Detail in Digital Musical Instrument Design (Thesis, Queen Mary \\nUniversity of London). Queen Mary University of London. Retrieved from \\nhttps://qmro.qmul.ac.uk/xmlui/handle/123456789/79852 ↩\\nAIMC 2024 (09/09 - 11/09 )\\nAugmenting the Expressivity of the Notochord Generative MIDI\\nModel for Arca\\'s \"The Light Comes in the Name of the Voice\"\\nMagnetic Resonator Piano Installation\\n20\\n5. McPherson, A., & E Kim, Y. (2012). The Problem of the Second Performer: Building a Community \\naround an Augmented Piano. Computer Music Journal, 36(4), 10–27. ↩\\n6. Armitage, J., Magnusson, T., & McPherson, A. (2023). Studying Subtle and Detailed Digital Lutherie: \\nMotivational Contexts and Technical Needs. Proc. New Interfaces for Musical Expression. Mexico City, \\nMexico. ↩\\n7. Pitkin, J. (2021). SoftMRP: A Software Emulation of the Magnetic Resonator Piano. International \\nConference on New Interfaces for Musical Expression. https://doi.org/10.21428/92fbeb44.9e7da18f ↩\\n8. Ji, S., Luo, J., & Yang, X. (2020). A Comprehensive Survey on Deep Music Generation: Multi-level \\nRepresentations, Algorithms, Evaluations, and Future Directions. arXiv:2011.06801 [Cs, Eess]. Retrieved \\nfrom http://arxiv.org/abs/2011.06801 ↩\\n9. Oore, S., Simon, I., Dieleman, S., Eck, D., & Simonyan, K. (2020). This time with feeling: learning \\nexpressive musical performance. Neural Computing and Applications, 32(4), 955–967. \\nhttps://doi.org/10.1007/s00521-018-3758-9 ↩\\n10. Dadman, S., & Bremdal, B. A. (2024). Crafting Creative Melodies: A User-Centric Approach for \\nSymbolic Music Generation. Electronics, 13(6), 1116. https://doi.org/10.3390/electronics13061116 ↩\\n11. Shepardson, V., & Privato, N. (2023). Notochord Arcs & Scrambled Signals. Proc. AI Music Creativity. \\nRetrieved from https://aimc2023.pubpub.org/pub/tii5jy7j ↩\\n12. Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., … Sui, Z. (2023). A Survey on In-context \\nLearning. arXiv. https://doi.org/10.48550/arXiv.2301.00234 ↩\\n13. Hsiao, W.-Y., Liu, J.-Y., Yeh, Y.-C., & Yang, Y.-H. (2021). Compound Word Transformer: Learning to \\nCompose Full-Song Music over Dynamic Directed Hypergraphs. arXiv. \\nhttps://doi.org/10.48550/arXiv.2101.02402 ↩\\n14. Shih, Y.-J., Wu, S.-L., Zalkow, F., Müller, M., & Yang, Y.-H. (2023). Theme Transformer: Symbolic \\nMusic Generation With Theme-Conditioned Transformer. IEEE Transactions on Multimedia, 25, 3495–\\n3508. https://doi.org/10.1109/TMM.2022.3161851 ↩\\n15. Jourdan, T., & Caramiaux, B. (2023). Machine Learning for Musical Expression: A Systematic \\nLiterature Review. New Interfaces for Musical Expression. ↩\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_file) as pdf:\n",
    "        for page_num in range(pdf.page_count):\n",
    "            page = pdf.load_page(page_num)  # Load each page\n",
    "            text += page.get_text()  # Extract text\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "pdf_path = './9p068non891yt8oeaielyxn8atc35f9l.pdf'\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "extracted_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95c49624-26f5-4025-b289-b7636c46ae2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'The model `gpt-4` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c24a4996a796>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtext_to_summarize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Your long text goes here...\"\u001b[0m  \u001b[0;31m# This can be from a PDF or text file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_to_summarize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-c24a4996a796>\u001b[0m in \u001b[0;36msummarize_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msummarize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     response = openai.completions.create(\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# You can also use gpt-3.5-turbo for cost efficiency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"You are a helpful assistant.\\nSummarize the following text:\\n{text}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/openai/resources/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, stream_options, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m     ) -> Completion | Stream[Completion]:\n\u001b[0;32m--> 539\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    540\u001b[0m             \u001b[0;34m\"/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             body=maybe_transform(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m         )\n\u001b[0;32m-> 1277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     def patch(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    952\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    955\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m         return self._process_response(\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'message': 'The model `gpt-4` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Set up your OpenAI API key\n",
    "openai.api_key = \"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\"  # Replace with your actual API key\n",
    "\n",
    "def summarize_text(text):\n",
    "    response = openai.completions.create(\n",
    "        model=\"gpt-4\",  # You can also use gpt-3.5-turbo for cost efficiency\n",
    "        prompt=f\"You are a helpful assistant.\\nSummarize the following text:\\n{text}\",\n",
    "        max_tokens=300,  # Adjust depending on how long the summary can be\n",
    "        temperature=0.7,  # Control randomness, 0 is deterministic, higher values give more creative outputs\n",
    "    )\n",
    "    \n",
    "    summary = response['choices'][0]['text'].strip()\n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "text_to_summarize = \"Your long text goes here...\"  # This can be from a PDF or text file\n",
    "summary = summarize_text(text_to_summarize)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1639f60-7e97-4b96-84d3-1c4b2ca85797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Grit CLI from https://github.com/getgrit/gritql/releases/latest/download/marzano-x86_64-unknown-linux-gnu.tar.gz\n",
      "\n",
      "\u001b[2K\u001b[1Am\u001b[0m \u001b[1m\u001b[2mFinding files                                                                 \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1Am\u001b[0m \u001b[1m\u001b[2mFinding files                                                                 \u001b[0m\u001b[1A\n",
      "\u001b[1m\u001b[2mAnalyzing\u001b[0m \u001b[1m\u001b[2mFinding files                                                         \u001b[0m\n",
      "\u001b[2K\u001b[2A░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 0/2\u001b[2A\n",
      "\u001b[1m\u001b[2mAnalyzing\u001b[0m \u001b[1m\u001b[2m./.ipynb_checkpoints/PaperSearch-checkpoint.ipynb                     \u001b[0m\n",
      "\u001b[2K\u001b[2A./.ipynb_checkpoints/PaperSearch-checkpoint.ipynb: ERROR (code: 280) - Section range 632-1525 is out of bounds inside 1523\n",
      "\n",
      "\u001b[1m\u001b[2mAnalyzing\u001b[0m \u001b[1m\u001b[2m./.ipynb_checkpoints/PaperSearch-checkpoint.ipynb                     \u001b[0m\n",
      "\u001b[2K\u001b[2A░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 0/2\u001b[2A\n",
      "\u001b[1m\u001b[2mAnalyzing\u001b[0m \u001b[1m\u001b[2m./.ipynb_checkpoints/PaperSearch-checkpoint.ipynb                     \u001b[0m\n",
      "\u001b[2K\u001b[2A██████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1/2\u001b[2A\n",
      "\u001b[1m\u001b[2mAnalyzing\u001b[0m \u001b[1m\u001b[2m./.ipynb_checkpoints/PaperSearch-checkpoint.ipynb                     \u001b[0m\n",
      "\u001b[2K\u001b[2A██████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1/2\u001b[2A\n",
      "\u001b[1m\u001b[2mAnalyzing\u001b[0m \u001b[1m\u001b[2m./PaperSearch.ipynb                                                   \u001b[0m\n",
      "\u001b[2K\u001b[2A./PaperSearch.ipynb: ERROR (code: 280) - Section range 632-1525 is out of bounds inside 1523\n",
      "\n",
      "\u001b[1m\u001b[2mAnalyzing\u001b[0m \u001b[1m\u001b[2m./PaperSearch.ipynb                                                   \u001b[0m\n",
      "\u001b[2K\u001b[2A██████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1/2\u001b[2A\n",
      "\u001b[1m\u001b[2mAnalyzing\u001b[0m \u001b[1m\u001b[2m./PaperSearch.ipynb                                                   \u001b[0m\n",
      "\u001b[2K\u001b[2A████████████████████████████████████████████████████████████████████ 2/2\u001b[2A\n",
      "\u001b[1m\u001b[2mAnalyzing\u001b[0m \u001b[1m\u001b[2m./PaperSearch.ipynb                                                   \u001b[0m\n",
      "\u001b[2K\u001b[2AProcessed 2 files and found 0 matches███████████████████████████████ 2/2\u001b[2A\n"
     ]
    }
   ],
   "source": [
    "!openai migrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a161f1fd-2a1b-43a3-a6d4-492fdc55cd53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
